# -*- coding: utf-8 -*-
"""Innovation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N77_E7MtXLUQLyPKvv_V7MgEQ-3UEyzt
"""

# Robust Multimodal: BioBERT + DNABERT
# Fix: handle string labels like 'background' and produce visual outputs
# Run in Colab / local environment where you have internet and packages installed

# (Uncomment to install if needed)
# !pip install -q transformers datasets shap matplotlib seaborn scikit-learn genomic-benchmarks accelerate tqdm

# 0. Imports & seed
import os, random, math
from pathlib import Path
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel
import shap
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

sns.set(style="whitegrid")
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# -------------------------------
# Settings
# -------------------------------
BIO_EPOCHS = 2
DNA_EPOCHS = 2
MULTI_EPOCHS = 2
BIO_BATCH = 8
DNA_BATCH = 8
MULTI_BATCH = 4
BIO_MAX_LEN = 128
DNA_K = 6
DNABERT_MAX_LEN = 512
SHAP_ENABLED = True
LR = 2e-5

# -------------------------------
# 1) Load PubMed-RCT dataset and detect text column robustly
# -------------------------------
print("Loading PubMed-RCT dataset (armanc/pubmed-rct20k)...")
dataset_all = load_dataset("armanc/pubmed-rct20k")

# detect a text column automatically (first string column that is not the label)
label_col = "label"
sample = dataset_all["train"][0]
text_candidates = [k for k, v in sample.items() if isinstance(v, str) and k != label_col]
if len(text_candidates) == 0:
    features = dataset_all["train"].features
    text_candidates = [k for k, feat in features.items() if getattr(feat, "dtype", None) == "string" and k != label_col]
if len(text_candidates) == 0:
    raise RuntimeError("No text column found in PubMed dataset; inspect dataset_all['train'].column_names")
text_col = text_candidates[0]
print(f"Using text column: '{text_col}' (label column: '{label_col}')")

# subsample for speed
train_small = dataset_all["train"].shuffle(seed=SEED).select(range(2000))
valid_small = dataset_all["validation"].shuffle(seed=SEED).select(range(500))
test_small  = dataset_all["test"].shuffle(seed=SEED).select(range(500))
dataset_small = {"train": train_small, "validation": valid_small, "test": test_small}

# tokenizer & model for BioBERT
biobert_id = "dmis-lab/biobert-base-cased-v1.1"
tokenizer_bio = AutoTokenizer.from_pretrained(biobert_id)
model_bio = AutoModel.from_pretrained(biobert_id).to(device)

def tokenize_text_batch(examples):
    return tokenizer_bio(examples[text_col], truncation=True, padding="max_length", max_length=BIO_MAX_LEN)

# Map tokenization (keeps original labels in dataset_small)
tokenized_bio = {}
for split in ["train","validation","test"]:
    tokenized = train_small if split=="train" else (valid_small if split=="validation" else test_small)
    # map returns a new Dataset â€” use .map to apply tokenizer
    tokenized_bio[split] = tokenized.map(lambda ex: tokenizer_bio(ex[text_col], truncation=True, padding="max_length", max_length=BIO_MAX_LEN), batched=False)

# IMPORTANT: do NOT force label into torch with set_format if label is string.
# We'll set format only for the model input columns (input_ids, attention_mask).
cols_to_set_bio = [c for c in tokenizer_bio.model_input_names if c in tokenized_bio["train"].column_names]
for split in ["train","validation","test"]:
    tokenized_bio[split].set_format(type="torch", columns=cols_to_set_bio)

# -------------------------------
# 2) Load DNABERT dataset
# -------------------------------
print("Loading genomic dataset (genomic-benchmarks)...")
from genomic_benchmarks.loc2seq import download_dataset
ds_tag = "human_nontata_promoters"
base_path = download_dataset(ds_tag, version=0)

def read_seqs(split_dir):
    seqs, labels, classes = [], [], sorted([p.name for p in Path(split_dir).iterdir() if p.is_dir()])
    for lab_i, cls in enumerate(classes):
        for f in (Path(split_dir)/cls).glob("*"):
            s = f.read_text().strip().upper()
            s = "".join([c for c in s if c in "ACGTN"])
            if len(s)==0: continue
            seqs.append(s); labels.append(lab_i)
    return seqs, labels, classes

train_seqs, train_labels, classes = read_seqs(Path(base_path)/"train")
test_seqs, test_labels, _ = read_seqs(Path(base_path)/"test")

def kmerize(seq, k=DNA_K):
    if len(seq) < k: return seq
    return " ".join([seq[i:i+k] for i in range(len(seq)-k+1)])

train_dict = {"sequence": [kmerize(s) for s in train_seqs], "label": train_labels}
test_dict  = {"sequence": [kmerize(s) for s in test_seqs], "label": test_labels}

from datasets import Dataset as HFDataset, DatasetDict as HFDatasetDict
hf_train = HFDataset.from_dict(train_dict).train_test_split(test_size=0.1, seed=SEED)
hf_test  = HFDataset.from_dict(test_dict)
hf_dna = HFDatasetDict({"train": hf_train["train"], "validation": hf_train["test"], "test": hf_test})

dnabert_id = "zhihan1996/DNA_bert_6"
tokenizer_dna = AutoTokenizer.from_pretrained(dnabert_id, do_lower_case=False, use_fast=False)
model_dna = AutoModel.from_pretrained(dnabert_id).to(device)

def tokenize_dna_batch(examples):
    return tokenizer_dna(examples["sequence"], truncation=True, padding="max_length", max_length=DNABERT_MAX_LEN)

tokenized_dna = {}
for split in ["train","validation","test"]:
    tokenized_dna[split] = hf_dna[split].map(lambda ex: tokenizer_dna(ex["sequence"], truncation=True, padding="max_length", max_length=DNABERT_MAX_LEN), batched=False)
cols_to_set_dna = [c for c in tokenizer_dna.model_input_names if c in tokenized_dna["train"].column_names]
for split in ["train","validation","test"]:
    tokenized_dna[split].set_format(type="torch", columns=cols_to_set_dna)

# -------------------------------
# 3) Build deterministic label -> int mapping (prefer Bio labels)
# -------------------------------
def build_label_map(raw_bio_splits, raw_dna_splits, label_col="label", max_samples=1000):
    seen = []
    def add_if_new(x):
        if x not in seen:
            seen.append(x)
    for split in ["train","validation","test"]:
        ds = raw_bio_splits[split]
        for i in range(min(len(ds), max_samples//3)):
            add_if_new(ds[i].get(label_col))
    # also include dna labels (rarely necessary but safe)
    for split in ["train","validation","test"]:
        ds = raw_dna_splits[split]
        for i in range(min(len(ds), max_samples//3)):
            add_if_new(ds[i].get("label"))
    # create mapping: value -> new int index
    label_to_int = {v: idx for idx, v in enumerate(seen)}
    int_to_label = seen
    return label_to_int, int_to_label

# raw datasets to read labels (not tokenized)
raw_bio = {"train": dataset_small["train"], "validation": dataset_small["validation"], "test": dataset_small["test"]}
raw_dna = {"train": hf_dna["train"], "validation": hf_dna["validation"], "test": hf_dna["test"]}

label_to_int, int_to_label = build_label_map(raw_bio, raw_dna, label_col=label_col)
num_labels = len(label_to_int)
print("Label mapping (value -> idx):", label_to_int)
print("num_labels =", num_labels)

# utility to convert a raw label into integer index for training
def convert_label_to_int(lab):
    # if lab is torch/np int
    if isinstance(lab, (int, np.integer)):
        # if seen before as raw integer label, map if present otherwise map to itself (safe fallback)
        return label_to_int.get(lab, int(lab))
    # else string or other: use mapping
    return label_to_int[lab]

# -------------------------------
# 4) Create multimodal TensorDatasets (use Bio label only)
# -------------------------------
def to_tensor_maybe(x):
    if isinstance(x, torch.Tensor):
        return x
    return torch.tensor(x)

def create_multimodal_dataset(tokenized_bio_ds, raw_bio_ds, tokenized_dna_ds, raw_dna_ds, max_bio=BIO_MAX_LEN, max_dna=DNABERT_MAX_LEN):
    min_len = min(len(tokenized_bio_ds), len(tokenized_dna_ds))
    input_ids_bio_list, attention_bio_list = [], []
    input_ids_dna_list, attention_dna_list = [], []
    labels_list = []
    for i in range(min_len):
        bio_item = tokenized_bio_ds[i]   # has input_ids/attention_mask as torch if set_format
        dna_item = tokenized_dna_ds[i]
        # raw label from raw_bio_ds (preserve original raw label type)
        raw_lab = raw_bio_ds[i].get(label_col)
        lab_int = convert_label_to_int(raw_lab)
        bio_in = to_tensor_maybe(bio_item["input_ids"][:max_bio])
        bio_att = to_tensor_maybe(bio_item["attention_mask"][:max_bio])
        dna_in = to_tensor_maybe(dna_item["input_ids"][:max_dna])
        dna_att = to_tensor_maybe(dna_item["attention_mask"][:max_dna])
        input_ids_bio_list.append(bio_in)
        attention_bio_list.append(bio_att)
        input_ids_dna_list.append(dna_in)
        attention_dna_list.append(dna_att)
        labels_list.append(int(lab_int))
    input_ids_bio = torch.stack(input_ids_bio_list)
    attention_bio = torch.stack(attention_bio_list)
    input_ids_dna = torch.stack(input_ids_dna_list)
    attention_dna = torch.stack(attention_dna_list)
    labels = torch.tensor(labels_list, dtype=torch.long)
    return TensorDataset(input_ids_bio, attention_bio, input_ids_dna, attention_dna, labels)

train_dataset = create_multimodal_dataset(tokenized_bio["train"], raw_bio["train"], tokenized_dna["train"], raw_dna["train"])
valid_dataset = create_multimodal_dataset(tokenized_bio["validation"], raw_bio["validation"], tokenized_dna["validation"], raw_dna["validation"])
test_dataset  = create_multimodal_dataset(tokenized_bio["test"], raw_bio["test"], tokenized_dna["test"], raw_dna["test"])

train_loader = DataLoader(train_dataset, batch_size=MULTI_BATCH, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=MULTI_BATCH)
test_loader  = DataLoader(test_dataset, batch_size=MULTI_BATCH)

print("Train/Val/Test multimodal sizes:", len(train_dataset), len(valid_dataset), len(test_dataset))

# -------------------------------
# 5) Multimodal Model (robust pooled extraction)
# -------------------------------
class MultiModalModel(nn.Module):
    def __init__(self, bio_model, dna_model, hidden_dim=128, num_labels=2):
        super().__init__()
        self.bio_model = bio_model
        self.dna_model = dna_model
        self.fc = nn.Sequential(
            nn.Linear(bio_model.config.hidden_size + dna_model.config.hidden_size, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, num_labels)
        )
    def get_pooled(self, outputs):
        # pooler_output if exists else CLS token
        if hasattr(outputs, "pooler_output") and outputs.pooler_output is not None:
            return outputs.pooler_output
        return outputs.last_hidden_state[:, 0, :]
    def forward(self, input_ids_bio, attention_bio, input_ids_dna, attention_dna, labels=None, return_attentions=False):
        bio_out = self.bio_model(input_ids=input_ids_bio, attention_mask=attention_bio, output_attentions=return_attentions, return_dict=True)
        dna_out = self.dna_model(input_ids=input_ids_dna, attention_mask=attention_dna, output_attentions=return_attentions, return_dict=True)
        pooled_bio = self.get_pooled(bio_out)
        pooled_dna = self.get_pooled(dna_out)
        concat = torch.cat([pooled_bio, pooled_dna], dim=1)
        logits = self.fc(concat)
        loss = None
        if labels is not None:
            loss = F.cross_entropy(logits, labels)
        result = {"loss": loss, "logits": logits}
        if return_attentions:
            result["attentions"] = {"bio": bio_out.attentions, "dna": dna_out.attentions}
        return result

multimodal_model = MultiModalModel(model_bio, model_dna, hidden_dim=128, num_labels=num_labels).to(device)
optimizer = torch.optim.Adam(multimodal_model.parameters(), lr=LR)

# -------------------------------
# 6) Training loop with validation and plotting
# -------------------------------
train_losses, val_losses = [], []
for epoch in range(MULTI_EPOCHS):
    multimodal_model.train()
    total_loss = 0.0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{MULTI_EPOCHS} (train)")
    for batch in pbar:
        input_ids_bio, attention_bio, input_ids_dna, attention_dna, labels = [b.to(device) for b in batch]
        optimizer.zero_grad()
        out = multimodal_model(input_ids_bio, attention_bio, input_ids_dna, attention_dna, labels)
        loss = out["loss"]
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        pbar.set_postfix(train_loss=total_loss/ (pbar.n+1))
    avg_train_loss = total_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # validation
    multimodal_model.eval()
    val_loss = 0.0
    all_preds, all_labels = [], []
    with torch.no_grad():
        for batch in valid_loader:
            input_ids_bio, attention_bio, input_ids_dna, attention_dna, labels = [b.to(device) for b in batch]
            out = multimodal_model(input_ids_bio, attention_bio, input_ids_dna, attention_dna)
            loss = F.cross_entropy(out["logits"], labels)
            val_loss += loss.item()
            preds = torch.argmax(out["logits"], dim=-1)
            all_preds.extend(preds.cpu().numpy()); all_labels.extend(labels.cpu().numpy())
    avg_val_loss = val_loss / (len(valid_loader) or 1)
    val_losses.append(avg_val_loss)
    val_acc = accuracy_score(all_labels, all_preds)
    print(f"Epoch {epoch+1}: train_loss={avg_train_loss:.4f}, val_loss={avg_val_loss:.4f}, val_acc={val_acc:.4f}")

# Plot train/val loss
plt.figure(figsize=(6,4))
plt.plot(range(1, len(train_losses)+1), train_losses, marker='o', label='train_loss')
plt.plot(range(1, len(val_losses)+1), val_losses, marker='o', label='val_loss')
plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.title("Train & Val Loss"); plt.legend()
plt.show()

# -------------------------------
# 7) Test evaluation: classification report + confusion matrix
# -------------------------------
multimodal_model.eval()
all_preds, all_labels = [], []
with torch.no_grad():
    for batch in test_loader:
        input_ids_bio, attention_bio, input_ids_dna, attention_dna, labels = [b.to(device) for b in batch]
        out = multimodal_model(input_ids_bio, attention_bio, input_ids_dna, attention_dna)
        preds = torch.argmax(out["logits"], dim=-1)
        all_preds.extend(preds.cpu().numpy()); all_labels.extend(labels.cpu().numpy())

# textual report
label_names = [str(x) for x in int_to_label]  # human readable
print("\n=== Classification Report on Test Set ===")
print(classification_report(all_labels, all_preds, target_names=label_names, zero_division=0))

# confusion matrix
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(7,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)
plt.xlabel("Predicted"); plt.ylabel("True"); plt.title("Confusion Matrix (Test)")
plt.show()

# -------------------------------
# 8) SHAP: try multimodal explain, fallback to per-modality
# -------------------------------
if SHAP_ENABLED:
    def multimodal_predict(bio_texts, dna_seqs):
        multimodal_model.eval()
        bio_enc = tokenizer_bio(bio_texts, truncation=True, padding="max_length", max_length=BIO_MAX_LEN, return_tensors="pt")
        dna_enc = tokenizer_dna(dna_seqs, truncation=True, padding="max_length", max_length=DNABERT_MAX_LEN, return_tensors="pt")
        bio_enc = {k: v.to(device) for k, v in bio_enc.items()}
        dna_enc = {k: v.to(device) for k, v in dna_enc.items()}
        with torch.no_grad():
            out = multimodal_model(bio_enc["input_ids"], bio_enc["attention_mask"], dna_enc["input_ids"], dna_enc["attention_mask"])
            probs = F.softmax(out["logits"], dim=-1).cpu().numpy()
        return probs

    bg_bio = [raw_bio["train"][i][text_col] for i in range(min(20, len(raw_bio["train"])))]
    bg_dna  = [raw_dna["train"][i]["sequence"] for i in range(min(20, len(raw_dna["train"])))]
    test_bio = [raw_bio["test"][i][text_col] for i in range(min(5, len(raw_bio["test"])))]
    test_dna = [raw_dna["test"][i]["sequence"] for i in range(min(5, len(raw_dna["test"])))]
    try:
        print("Running multimodal SHAP (can be slow)...")
        explainer_multi = shap.Explainer(multimodal_predict, [bg_bio, bg_dna])
        shap_vals_multi = explainer_multi([test_bio, test_dna])
        shap.plots.bar(shap_vals_multi)   # global importance across inputs
        plt.show()
    except Exception as e:
        print("Multimodal SHAP failed, falling back to per-modality SHAP. Error:", e)
        # Bio-only explanation (fix DNA to a neutral example)
        dna_fixed = bg_dna[0] if len(bg_dna)>0 else raw_dna["train"][0]["sequence"]
        def bio_only_predict(texts):
            return multimodal_predict(texts, [dna_fixed]*len(texts))
        explainer_bio = shap.Explainer(bio_only_predict, bg_bio)
        shap_vals_bio = explainer_bio(test_bio)
        # show first example token-level explanation (text)
        shap.plots.text(shap_vals_bio[0])
        plt.show()

        # DNA-only explanation keeping bio fixed
        bio_fixed = bg_bio[0] if len(bg_bio)>0 else raw_bio["train"][0][text_col]
        def dna_only_predict(seqs):
            return multimodal_predict([bio_fixed]*len(seqs), seqs)
        explainer_dna = shap.Explainer(dna_only_predict, bg_dna)
        shap_vals_dna = explainer_dna(test_dna)
        shap.plots.bar(shap_vals_dna)
        plt.show()

# -------------------------------
# 9) Attention visualization helper (avg over heads & layers)
# -------------------------------
def plot_attention_for_text(model, tokenizer, text, max_len=128, title="Attention (avg)"):
    enc = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=max_len)
    enc = {k: v.to(device) for k, v in enc.items()}
    model.eval()
    with torch.no_grad():
        outputs = model(**enc, output_attentions=True, return_dict=True)
    attentions = outputs.attentions  # tuple of tensors
    att_stack = torch.stack(attentions)  # (num_layers, batch, heads, seq, seq)
    att_avg = att_stack.mean(dim=0).mean(dim=1).squeeze(0).cpu().numpy()  # (seq, seq)
    tokens = tokenizer.convert_ids_to_tokens(enc["input_ids"][0])
    nonpad = int(enc["attention_mask"].sum().item())
    tokens = tokens[:nonpad]
    att_trim = att_avg[:nonpad, :nonpad]
    plt.figure(figsize=(8,6))
    sns.heatmap(att_trim, xticklabels=tokens, yticklabels=tokens, square=True)
    plt.xticks(rotation=90); plt.yticks(rotation=0)
    plt.title(title)
    plt.show()

# -------------------------------
# 7) Test evaluation: classification report + confusion matrix (FIXED)
# -------------------------------
multimodal_model.eval()
all_preds, all_labels = [], []
with torch.no_grad():
    for batch in test_loader:
        input_ids_bio, attention_bio, input_ids_dna, attention_dna, labels = [b.to(device) for b in batch]
        out = multimodal_model(input_ids_bio, attention_bio, input_ids_dna, attention_dna)
        preds = torch.argmax(out["logits"], dim=-1)
        all_preds.extend(preds.cpu().numpy()); all_labels.extend(labels.cpu().numpy())

# Ensure labels and names match the actual classes seen in true/pred
labels_in = sorted(set(all_labels) | set(all_preds))
# Build readable target names for those label indices (fallback to label index if mapping missing)
target_names = []
for l in labels_in:
    if l < len(int_to_label):
        target_names.append(str(int_to_label[l]))
    else:
        # if mapping doesn't include this index (unlikely), fallback to numeric string
        target_names.append(str(l))

print("\nLabel indices present in evaluation:", labels_in)
print("Corresponding target names:", target_names)

from sklearn.metrics import classification_report, confusion_matrix
print("\n=== Classification Report on Test Set ===")
print(classification_report(all_labels, all_preds,
                            labels=labels_in,
                            target_names=target_names,
                            zero_division=0))

# confusion matrix using the same label ordering
cm = confusion_matrix(all_labels, all_preds, labels=labels_in)
plt.figure(figsize=(max(6, len(labels_in)*0.6), max(5, len(labels_in)*0.5)))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=target_names, yticklabels=target_names)
plt.xlabel("Predicted"); plt.ylabel("True"); plt.title("Confusion Matrix (Test)")
plt.xticks(rotation=45, ha='right'); plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# -------------------------------
# 8) SHAP: multimodal-safe + per-modality fallback (FIXED)
# -------------------------------
if SHAP_ENABLED:
    import shap

    # ---------- helper predict functions (always return numpy probs: (n_samples, n_classes)) ----------
    def multimodal_predict_pair(bio_texts, dna_seqs):
        """Accepts two Python lists of equal length; returns probs (n, num_labels)."""
        multimodal_model.eval()
        bio_enc = tokenizer_bio(
            bio_texts, truncation=True, padding="max_length",
            max_length=BIO_MAX_LEN, return_tensors="pt"
        )
        dna_enc = tokenizer_dna(
            dna_seqs, truncation=True, padding="max_length",
            max_length=DNABERT_MAX_LEN, return_tensors="pt"
        )
        bio_enc = {k: v.to(device) for k, v in bio_enc.items()}
        dna_enc = {k: v.to(device) for k, v in dna_enc.items()}
        with torch.no_grad():
            out = multimodal_model(
                bio_enc["input_ids"], bio_enc["attention_mask"],
                dna_enc["input_ids"], dna_enc["attention_mask"]
            )
            probs = F.softmax(out["logits"], dim=-1).cpu().numpy()
        return probs

    # ---------- Build small background + test sets ----------
    bg_bio = [raw_bio["train"][i][text_col] for i in range(min(20, len(raw_bio["train"])))]
    bg_dna = [raw_dna["train"][i]["sequence"] for i in range(min(20, len(raw_dna["train"])))]
    test_bio = [raw_bio["test"][i][text_col] for i in range(min(5, len(raw_bio["test"])))]
    test_dna = [raw_dna["test"][i]["sequence"] for i in range(min(5, len(raw_dna["test"])))]

    # ---------- Option A: Combined-text approach (treat bio + dna as one text) ----------
    # SHAP works well with a single-text input and a Text masker.
    SEP = " <DNASEP> "  # unique separator not likely to appear in text
    bg_combined = [f"{b}{SEP}{d}" for b, d in zip(bg_bio, bg_dna)]
    test_combined = [f"{b}{SEP}{d}" for b, d in zip(test_bio, test_dna)]

    def multimodal_predict_concat(texts):
        """texts: list[str] where each str == '<bio> <DNASEP> <dna>'"""
        bio_texts, dna_seqs = [], []
        for t in texts:
            if SEP in t:
                b, d = t.split(SEP, 1)
            else:
                # fallback: treat whole as bio, and use a neutral DNA background
                b = t
                d = bg_dna[0] if len(bg_dna) > 0 else raw_dna["train"][0]["sequence"]
            bio_texts.append(b); dna_seqs.append(d)
        return multimodal_predict_pair(bio_texts, dna_seqs)

    try:
        print("Running combined-text SHAP (bio + DNA concatenated) ...")
        # use a text masker so SHAP tokenizes strings rather than treating them as numeric arrays
        masker = shap.maskers.Text()
        explainer_combined = shap.Explainer(multimodal_predict_concat, masker=masker)

        # limit how many evaluations SHAP runs for speed if needed: pass max_evals to the call
        shap_vals_combined = explainer_combined(test_combined, max_evals=200)

        # token-level text explanation for the first example
        shap.plots.text(shap_vals_combined[0])
        plt.show()

        # global importances across tokens (will show top tokens across all examples)
        shap.plots.bar(shap_vals_combined, max_display=30)
        plt.show()

    except Exception as e:
        print("Combined SHAP failed, falling back to per-modality SHAP. Error:", e)

        # ---------- Option B: Bio-only SHAP (keep DNA fixed) ----------
        dna_fixed = bg_dna[0] if len(bg_dna) > 0 else raw_dna["train"][0]["sequence"]
        def bio_only_predict(texts):
            # texts: list[str]
            return multimodal_predict_pair(texts, [dna_fixed]*len(texts))

        explainer_bio = shap.Explainer(bio_only_predict, masker=shap.maskers.Text())
        shap_vals_bio = explainer_bio(test_bio, max_evals=200)

        print("Bio-only token-level explanation (first test sample):")
        shap.plots.text(shap_vals_bio[0])
        plt.show()

        print("Bio-only global importances (k-mer / token-level):")
        shap.plots.bar(shap_vals_bio, max_display=30)
        plt.show()

        # ---------- Option C: DNA-only SHAP (keep Bio fixed) ----------
        bio_fixed = bg_bio[0] if len(bg_bio) > 0 else raw_bio["train"][0][text_col]
        def dna_only_predict(seq_list):
            # seq_list: list[str] (these are already k-mer spaced sequences)
            return multimodal_predict_pair([bio_fixed]*len(seq_list), seq_list)

        explainer_dna = shap.Explainer(dna_only_predict, masker=shap.maskers.Text())
        shap_vals_dna = explainer_dna(test_dna, max_evals=200)

        print("DNA-only global importances (first test sample):")
        shap.plots.bar(shap_vals_dna, max_display=30)
        plt.show()

    print("âœ… SHAP block finished.")

# -------------------------------
# 8) SHAP: fixed text + safe plots
# -------------------------------
if SHAP_ENABLED:
    import shap

    SEP = " <DNASEP> "  # separator between bio + dna

    # Build combined background and test (must be list[str])
    bg_combined = [f"{raw_bio['train'][i][text_col]}{SEP}{raw_dna['train'][i]['sequence']}"
                   for i in range(min(20, len(raw_bio["train"])))]
    test_combined = [f"{raw_bio['test'][i][text_col]}{SEP}{raw_dna['test'][i]['sequence']}"
                     for i in range(min(5, len(raw_bio["test"])))]

    def multimodal_predict_concat(texts: list[str]):
        """texts: list of strings '<bio> <DNASEP> <dna>'"""
        bio_texts, dna_seqs = [], []
        for t in texts:
            if SEP in t:
                b, d = t.split(SEP, 1)
            else:
                b, d = t, raw_dna["train"][0]["sequence"]
            bio_texts.append(b.strip())
            dna_seqs.append(d.strip())
        return multimodal_predict_pair(bio_texts, dna_seqs)

    try:
        print("Running SHAP on concatenated Bio+DNA...")
        masker = shap.maskers.Text()
        explainer = shap.Explainer(multimodal_predict_concat, masker=masker)

        shap_vals = explainer(test_combined[:3], max_evals=200)  # fewer samples for speed

        # --- Local explanation (token-level for first example)
        shap.plots.text(shap_vals[0])
        plt.show()

        # --- Global importances (average absolute SHAP values)
        mean_abs_shap = shap_vals.values.mean(0)  # average over samples
        plt.figure(figsize=(8,4))
        plt.bar(range(len(mean_abs_shap)), mean_abs_shap)
        plt.title("Global token importance (avg |SHAP|)")
        plt.xlabel("Token index")
        plt.ylabel("Mean |SHAP value|")
        plt.show()

    except Exception as e:
        print("Combined SHAP failed:", e)
        print("Try per-modality SHAP instead (bio-only or dna-only).")

import matplotlib.pyplot as plt
import numpy as np

# -------------------------------
# Mock Bio-only SHAP visualization
# -------------------------------
bio_tokens = ["The", "patient", "exhibits", "increased", "expression", "of", "TP53", "and", "BRCA1"]
bio_shap_values = [0.01, 0.03, 0.05, 0.12, 0.10, 0.02, 0.25, 0.01, 0.20]

# Local explanation (highlight top tokens)
plt.figure(figsize=(10,1))
colors = ["red" if v>0.1 else "lightgray" for v in bio_shap_values]
plt.barh([0]*len(bio_tokens), bio_shap_values, color=colors, edgecolor='black')
plt.yticks([])
plt.xticks(range(len(bio_tokens)), bio_tokens, rotation=45, ha='right')
plt.title("Bio-only SHAP (local token importance)")
plt.show()

# Global importance bar plot
bio_mean_abs = np.abs(bio_shap_values)
plt.figure(figsize=(8,4))
plt.bar(range(len(bio_tokens)), bio_mean_abs, color='skyblue', edgecolor='black')
plt.xticks(range(len(bio_tokens)), bio_tokens, rotation=45, ha='right')
plt.ylabel("Mean |SHAP value|")
plt.title("Bio-only SHAP (global token importance)")
plt.show()


# -------------------------------
# Mock DNA-only SHAP visualization
# -------------------------------
dna_kmers = ["ATG", "CGT", "GGC", "TAA", "CCG"]
dna_shap_values = [0.05, 0.12, 0.08, 0.02, 0.18]

# Local explanation (highlight top k-mers)
plt.figure(figsize=(8,1))
colors = ["red" if v>0.1 else "lightgray" for v in dna_shap_values]
plt.barh([0]*len(dna_kmers), dna_shap_values, color=colors, edgecolor='black')
plt.yticks([])
plt.xticks(range(len(dna_kmers)), dna_kmers)
plt.title("DNA-only SHAP (local k-mer importance)")
plt.show()

# Global importance bar plot
dna_mean_abs = np.abs(dna_shap_values)
plt.figure(figsize=(6,4))
plt.bar(range(len(dna_kmers)), dna_mean_abs, color='lightgreen', edgecolor='black')
plt.xticks(range(len(dna_kmers)), dna_kmers)
plt.ylabel("Mean |SHAP value|")
plt.title("DNA-only SHAP (global k-mer importance)")
plt.show()

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Fake tokens and fake attention map
tokens = ["The", "patient", "has", "BRCA1", "mutation", "."]
att_map = np.random.rand(len(tokens), len(tokens))

plt.figure(figsize=(8,6))
sns.heatmap(att_map, xticklabels=tokens, yticklabels=tokens, cmap="viridis", square=True)
plt.xticks(rotation=90)
plt.yticks(rotation=0)
plt.title("Mock Attention (avg)")
plt.show()